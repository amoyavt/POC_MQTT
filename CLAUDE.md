# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Architecture

This is a secure, production-ready IoT data pipeline that ingests sensor data from F2 Smart Controller devices, transforming raw MQTT messages into structured time-series data for analytics and monitoring. The system implements mTLS authentication, stream processing with Kafka, and high-performance data storage with TimescaleDB.

**Key Features:**
- üîí **Security First**: mTLS authentication, ACL-based authorization, non-root containers
- ‚ö° **High Performance**: Optimized batching, connection pooling, Redis caching
- üìä **Real-time Processing**: Stream processing with Kafka and TimescaleDB
- üêõ **Developer Friendly**: Comprehensive logging, health checks, easy debugging
- üìà **Production Ready**: Monitoring, metrics, horizontal scaling support

### Core Components

1. **Certificate Generation Service** (`services/certgen_api/`): FastAPI service that generates X.509 certificates for devices and brokers using a CA infrastructure
2. **MQTT Broker** (`services/mqtt_broker/`): Mosquitto broker configured with mTLS authentication and ACL-based authorization
3. **FACES2 Controllers Simulator** (`services/faces2_controllers/`): Python simulator for F2 Smart Controller devices that publish sensor data
4. **MQTT-Kafka Connector** (`services/mqtt_kafka_connector/`): Bridges MQTT messages to Kafka topics for stream processing
5. **Apache Kafka + Zookeeper**: Stream processing with topics for raw and processed IoT data
6. **Stream Processor** (`services/stream_processor/`): Real-time data transformation with MAC‚Üídevice_id mapping
7. **PostgreSQL Database**: Metadata storage for device and datapoint mappings
8. **Redis Cache**: High-speed caching for processor lookups
9. **TimescaleDB**: Time-series data storage with optimized decoded_data table
10. **Kafka-TimescaleDB Sink** (`services/kafka_timescale_sink/`): Batch processing from Kafka to TimescaleDB
11. **Shared Models** (`services/shared/`): Pydantic models for data validation and transformation across services

### Network Architecture

All services communicate through the `iot-network` Docker bridge network.
- FACES2 controllers authenticate to MQTT broker using device certificates
- Broker validates devices using CA certificate
- All certificates are generated by the certgen-api service
- Services communicate internally via Docker bridge network without external credentials

### Data Flow

1. F2 Smart Controllers request certificates from certgen-api during startup
2. Controllers connect to MQTT broker using mTLS authentication
3. Messages are published to topic structure: `<TOPIC_TYPE>/f2-<MAC_ADDR>/<MODE>/<CONNECTOR>/<COMPONENT>`
4. MQTT-Kafka connector streams messages to `raw-iot-data` Kafka topic
5. Stream processor consumes raw data, performs MAC‚Üídevice_id mapping, and produces to `processed-iot-data` topic
6. Kafka-TimescaleDB sink consumes processed data and batch inserts to TimescaleDB `decoded_data` table
7. PostgreSQL stores metadata for device and datapoint mappings
8. Redis provides high-speed caching for lookup operations

**Architecture Reference**: See the Mermaid diagram in README.md for a visual representation of the complete data pipeline.

## Common Commands

### Docker Operations
```bash
# Start all services
docker-compose up -d

# View logs for specific service
docker-compose logs -f <service-name>

# Rebuild specific service
docker-compose up -d --build <service-name>

# Stop all services
docker-compose down
```

### Certificate Management
The CA and issued certificates are stored in:
- `./ca/`: CA certificate, private key, and OpenSSL configuration
- `./issued/`: Device certificates and keys

## Environment Variables

All environment variables from `.env`:

### Certificate Configuration
- `CA_CN`: Common Name for the CA (default: IoT_CA)
- `CA_EXPIRY`: CA validity in days (default: 3650)
- `BROKER_CN`: Common Name for the MQTT broker (default: mqtt-broker)
- `BROKER_EXPIRY`: Broker certificate validity in days (default: 3650)
- `DEVICE_EXPIRY`: Device certificate validity in days (default: 365)

### Service Configuration
- `OPENSSL_CNF`: Path to OpenSSL configuration file (/ca/openssl.cnf)
- `CA_KEY`: Path to CA private key (/ca/ca.key)
- `CA_CERT`: Path to CA certificate (/ca/ca.crt)
- `MQTT_BROKER_HOST`: MQTT broker hostname (mqtt-broker)
- `CERTGEN_API_HOST`: Certificate generation API hostname (certgen-api)
- `PUBLISH_INTERVAL`: Sensor data publishing interval in seconds (default: 5)

### Database Configuration
- `POSTGRES_USER`: PostgreSQL username (default: postgres)
- `POSTGRES_PASSWORD`: PostgreSQL password (default: password)
- `POSTGRES_DB`: PostgreSQL database name (default: db)
- `POSTGRES_HOST`: PostgreSQL hostname (default: localhost)
- `POSTGRES_PORT`: PostgreSQL port (default: 5432)
- `TIMESCALE_USER`: TimescaleDB username (default: timescale)
- `TIMESCALE_PASSWORD`: TimescaleDB password (default: password)
- `TIMESCALE_DB`: TimescaleDB database name (default: timeseries)
- `TIMESCALE_HOST`: TimescaleDB hostname (default: timescaledb)
- `REDIS_HOST`: Redis hostname (default: redis)
- `REDIS_PORT`: Redis port (default: 6379)

### Kafka Configuration
- `KAFKA_BOOTSTRAP_SERVERS`: Kafka bootstrap servers (default: kafka:9092)
- `KAFKA_RAW_TOPIC`: Raw IoT data topic (default: raw-iot-data)
- `KAFKA_PROCESSED_TOPIC`: Processed IoT data topic (default: processed-iot-data)

### Processing Configuration
- `BATCH_SIZE`: Batch size for processing (default: 1000)
- `PROCESSING_INTERVAL`: Processing interval in seconds (default: 5)
- `FLUSH_INTERVAL`: Flush interval for batching (default: 10)

## Development Notes

### Adding New Services

When adding services that need MQTT connectivity:
1. Add certificate provisioning logic similar to faces2_controllers (only for physical devices)
2. Configure mTLS in MQTT client setup (only for physical devices)
3. Add appropriate ACL rules in `services/mqtt_broker/acl`
4. Use shared models from `services/shared/models.py` for data validation
5. Connect to `iot-network` bridge network for internal communication

When adding services that need data processing:
1. Use DecodedData model for TimescaleDB operations
2. Use DeviceLookup and DataPointLookup models for metadata operations
3. Implement Redis caching for performance optimization
4. Follow batch processing patterns for high throughput
5. Use PostgreSQL for metadata storage and TimescaleDB for time-series data

### MQTT Topic Structure

Follow the established pattern: `<TOPIC_TYPE>/f2-<MAC_ADDR>/<MODE>/<CONNECTOR>/<COMPONENT>`
- `TOPIC_TYPE`: `cmnd` (commands), `stat` (status), `tele` (telemetry)
- Device MAC addresses are used as unique identifiers
- See `docs/mqtt_topics.md` for complete topic specification

### Security Considerations

- FACES2 controllers use mTLS because they simulate physical devices
- MAC addresses are sanitized and validated in certificate generation
- Non-root containers are used throughout
- Secrets are managed through Docker volumes, not environment variables
- Services communicate internally via Docker bridge network without external credentials

### Data Models

Key models in `services/shared/models.py`:

1. **DecodedData**: Optimized TimescaleDB storage model with 4 columns (timestamp, device_id, datapoint_id, value)
2. **DeviceLookup**: MAC address to device_id mapping for PostgreSQL metadata
3. **DataPointLookup**: Sensor label to datapoint_id mapping for PostgreSQL metadata
4. **CertificateRequest**: Validates certificate issuance requests with MAC address validation  
5. **CertificateResponse**: Structures certificate response with client cert, private key, CA cert, and expiration

The `DecodedData` model uses integer IDs for space optimization in TimescaleDB storage. The lookup models enable efficient mapping from MAC addresses and sensor labels to integer IDs for better performance.

### TimescaleDB Schema

The `decoded_data` table structure (space-optimized):
```sql
CREATE TABLE decoded_data (
    timestamp TIMESTAMPTZ NOT NULL,
    device_id INTEGER NOT NULL,
    datapoint_id INTEGER NOT NULL,
    value DOUBLE PRECISION,
    PRIMARY KEY (timestamp, device_id, datapoint_id)
);
```

This schema uses integer IDs instead of string labels for significant space savings and improved query performance.

### Kafka Log Compaction

The `processed-iot-data` topic uses log compaction to serve as a durable, queryable source for the "last known state" of every data point:

**Configuration:**
- `cleanup.policy=compact`: Retains only the latest message per key
- `min.cleanable.dirty.ratio=0.1`: More aggressive compaction (default: 0.5)
- `segment.ms=60000`: Faster segment creation for quicker compaction

**Composite Keying:**
- Stream processor uses `{device_id}:{datapoint_id}` format (e.g., `"5:1"`)
- Ensures each sensor data point has a unique key for log compaction
- Enables efficient last-known-state queries by reading entire topic

**Automatic Configuration:**
- Kafka service automatically applies log compaction settings on startup
- Configuration survives container restarts and rebuilds

## Docker Development Troubleshooting

### Cross-Platform Line Ending Issues

**Problem:** Scripts created/edited on Windows have CRLF (`\r\n`) line endings that break bash execution in Linux containers.

**Symptoms:**
- `exec /path/to/script.sh: no such file or directory` errors
- Scripts appear to exist but fail to execute

**Solutions:**
1. **Fix locally in WSL:** `sed -i 's/\r$//' path/to/script.sh`
2. **IDE settings:** In VSCode, click CRLF in bottom-right ‚Üí select LF
3. **Use dos2unix:** `dos2unix path/to/script.sh` (if installed)

### Container Permission Issues

**Problem:** Different Docker base images have varying security models and file system permissions.

**Symptoms:**
- `chmod: changing permissions of '/path/file': Operation not permitted`
- `mkdir: cannot create directory '/path': Permission denied`
- `sed: couldn't open temporary file: Permission denied`

**Solutions:**
1. **Use COPY --chmod:** `COPY --chmod=755 script.sh /path/script.sh`
2. **Avoid restricted paths:** Some containers restrict `/usr/local/bin/`, `/app/`, `/home/`
3. **Check base image docs:** Different images have different writeable directories

### Docker Build Best Practices

**Key Principles:**
1. **Fix issues at source:** Handle platform-specific problems in development environment, not in containers
2. **Use proper build contexts:** Keep related files in same directory as Dockerfile
3. **Test with clean builds:** Use `--no-cache` and `docker-compose down -v` for troubleshooting
4. **Read error messages carefully:** They often reveal the exact root cause

**Common Patterns:**
```dockerfile
# Good: Handle line endings and permissions cleanly
FROM base-image
COPY --chmod=755 script.sh /path/script.sh
ENTRYPOINT ["/path/script.sh"]

# Avoid: Trying to fix line endings in container
FROM base-image
COPY script.sh /path/script.sh
RUN dos2unix /path/script.sh && chmod +x /path/script.sh  # May fail
```

**Testing Strategy:**
1. Test each fix individually, not multiple changes at once
2. Use `docker-compose down -v` to ensure clean state
3. Verify end-to-end functionality after fixes
4. Document working solutions for future reference

### Shell Compatibility

**Issue:** Docker RUN commands default to `/bin/sh`, not `/bin/bash`.

**Solutions:**
1. **Use SHELL directive:** `SHELL ["/bin/bash", "-c"]`
2. **Write sh-compatible commands:** Avoid bash-specific syntax
3. **Test commands in both shells:** Ensure compatibility

**Example:**
```dockerfile
# If you need bash features
SHELL ["/bin/bash", "-c"]
RUN complex_bash_command

# Or keep commands simple for /bin/sh compatibility
RUN simple_command
```